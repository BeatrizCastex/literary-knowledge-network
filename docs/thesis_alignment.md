# Thesis Alignment Notes

This document connects the pipeline implementation to the thesis objectives and deliverables.  

## Research Questions & Pipeline Support

### 1. How can Goodreads metadata be transformed into a structured knowledge graph?
- **Steps 1-4 (`data-extraction-v3.py`, `data-normalization.py`, `enritch-data.py`, `data-injection.py`)** ingest, clean, enrich, and load the data into Neo4j.
- The process normalizes identifiers, harmonizes publishers/tags, and augments entities with Wikidata attributes.
- `config.yaml` centralizes parameters so experiments are reproducible and easy to cite.

### 2. What methods capture literary similarity at scale?
- **Step 5 (`embedding.py`)** generates SBERT and TF-IDF representations, providing complementary views of thematic relatedness.
- **Step 6 (`similarity.py`)** combines the embeddings into a weighted similarity graph, parameterized by `top_k`, `threshold`, and TF-IDF blending.
- Outputs are tracked in `data/output/embeddings/`, `data/output/similarity/`, and the CSV edge lists in `data/output/edges/`; summaries are logged to JSON for later inspection.

### 3. Can communities reveal interpretable clusters for literary analysis?
- **Step 7 (`community_detection.py`)** runs Leiden community detection on the similarity graph.
- Detected clusters, their sizes, and metadata are persisted both to Parquet (`data/output/communities/`) and Neo4j node properties.
- Notebook templates in `notebooks/` (e.g., `cluster_explorer.ipynb`) help translate clusters into thesis-ready figures.

## Deliverables Mapped to Repository Assets

- **Literary knowledge graph:** Neo4j database populated via `data-injection.py`; schema documented in `docs/neo4j_data_dictionary.md`.
- **10k-node similarity network:** Generated by running `./run_pipeline.sh 10000 42 <password>`; export Parquet files under `data/output/similarity/` and CSV edge lists from `data/output/edges/`.
- **Incremental sampling utilities:** `data-extraction-v3.py` can top up an existing dataset via `--existing-work-file --target-after-filters --incremental-only`, exporting the additional works for focused normalization/enrichment.
- **Community narratives:** Derived from `data/output/communities/work_communities.parquet` / `.csv` and explored in the provided notebooks.
- **Reproducibility evidence:** JSON logs in `logs/pipeline_runs/` (sample size, runtime, config hash) plus `docs/reproducibility.md` checklist and the consolidated metrics bundle (`data/output/results_metrics.json`).
- **Published thesis link (when available):** Add citation details to `CITATION.cff` and reference this document in the README.

## Release Checklist

1. Run the full pipeline with the desired sample size (10,000 for the thesis release).
2. If additional works are required, top up via `data-extraction-v3.py --incremental-only --existing-work-file ... --target-after-filters ...` and re-run Steps 2â€“3 on the incremental export before merging results.
3. Export any artefacts you intend to share (e.g., filtered similarity edges, figures, aggregated metrics) outside the git-ignored directories.
4. Update `CITATION.cff` with the final repository URL and version tag.
5. Review notebooks to ensure cached outputs or private paths are cleared before commit.
6. Tag the release and archive the repository snapshot for submission.
